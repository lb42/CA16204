<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title/>
                <author/>
            </titleStmt>
            <editionStmt>
                <edition><date>2017-11-04</date></edition>
            </editionStmt>
            <publicationStmt>
                <p>unknown</p>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application xml:id="docxtotei" ident="TEI_fromDOCX" version="2.15.0">
                    <label>DOCX to TEI</label>
                </application>
            </appInfo>
        </encodingDesc>
        <revisionDesc>
            <listChange>
                <change><date>2018-01-27T12:33:43Z</date><name/></change>
            </listChange>
        </revisionDesc>
    </teiHeader>
    <text>
        <body>
            <div>
                <head>AC16204, WG1 – Workflow</head>
                <p> The objective of this document is to set up the main steps to build the ELTeC
                    core corpus of novels.</p>
                <p> The corpus will include novels written during 1850 and 1920 in these l<anchor
                        type="commentRangeStart" n="0"/>anguages<anchor type="commentRangeEnd" n="0"
                        /><note place="comment" resp="CO" n="0"><date when="2018-01-24T10:42:31Z"
                            /><hi>How do we would like to organize the work?Building subgroups for a
                            specific language or a specific step? How do we want to make decisions
                            and discuss problems (with annotations, distinct steps or novels)For our
                            meeting in Prague, I plan an organisational part with respect to WG
                            communication and planning based on the meeting of WG4</hi></note>:
                    Dutch, English, French, German, Modern Greek, Italian, Polish, Portuguese,
                    Russian and Spanish. Each novel will be in a machine readable format and encoded
                    in XML (following standar TEI).</p>
                <p> The main steps to achieve this objective are the next:</p>
                <list type="ordered">
                    <item>Selecting authors and novels.</item>
                    <item>Finding the novels.</item>
                    <item>Cleaning and normalizing texts.</item>
                    <item>Annotation.</item>
                    <item>Publication.</item>
                    <item>Evaluation.</item>
                </list>
                <div>
                    <head>Starting point</head>
                    <p>The starting point are three documents:</p>
                    <p> Doc1: <hi rend="italic">Sampling Criteria</hi>, in which the main
                        requirements for text selection are established.</p>
                    <p> Doc2: <hi rend="italic">Encoding Guideline</hi>, in which the encoding
                        scheme is defined.</p>
                    <p> Doc3: <hi rend="italic">Workflow</hi>, this document.</p>
                </div>
                <div>
                    <head>Step 1. Selecting authors and novels.</head>
                    <p> The objective of this step is to find appropriate <anchor
                            type="commentRangeStart" n="1"/><anchor type="commentRangeStart" n="2"
                            />novels<anchor type="commentRangeEnd" n="2"/><note place="comment"
                            resp="LB" n="2"><date when="2018-01-24T11:00:04Z"/><hi>I agree with
                                Carolin’s comment below: the selection process is a matter of
                                listing candiadte texts to be chosen according to the selection
                                criteria identified in doc1 : no claim for representativeness
                                needed.</hi></note><note place="comment" resp="BN" n="3"><date
                                when="2018-01-25T10:17:59Z"/><hi>Respuesta a Unknown Author
                                (24/01/2018, 11:00): "..."</hi><hi style="font-size:10pt"
                            >ok</hi></note><anchor type="commentRangeEnd" n="1"/><note
                            place="comment" resp="CO" n="1"><date when="2018-01-24T09:38:13Z"
                                /><hi>CO: Representativeness refers to the extent to which a sample
                                includes the full range of variability in a population. We don’t
                                know every book of every language published/read/discussed in the
                                period in question. It is further ‘impossible to identify a complete
                                list of ‘categories’ that would exhaustively account for all texts
                                produced in a given language’We should use this word more carefully.
                            </hi></note><note place="comment" resp="BN" n="4"><date
                                when="2018-01-25T10:17:53Z"/><hi>Respuesta a Unbekannter Autor
                                (24/01/2018, 09:38): "..."</hi><hi style="font-size:10pt"
                            >ok</hi></note> of each language published during 1850-1920 according to
                        the sampling criteria (Doc1). It will be done in two sub-steps. The firs one
                        is to extract, as far as possible, a list of novels published during that
                        period and the amount of reprints. This information<note place="comment"
                            resp="LB" n="5"><date when="2018-01-24T11:01:34Z"/><hi>As far as I have
                                looked the OCLC WorldCat contains most of what we need</hi></note>
                        could be extracted from different sources as <hi
                            rend="italic bold strikethrough doublestrikethrough"
                            style="font-size:11pt">OCLC WorldCat or specific</hi> National Libraries
                        of each Country. See Apendix A for a list of catalogs. Each country will
                        complete this list of catalogs with specific sources.</p>
                    <p> The list of novels will be published in a web page, including information
                        about author’s name, title, date and place of the first edition, number of
                        reprints during the period, source from which this information has been
                        extracted, size, topic, etc.. See eltecSheet document. At this moment, the
                        information that must be stored is not fixed. We expect suggestions from the
                        all group.</p>
                    <p> The second sub-step is the selection of the appropriate novels for the ELTeC
                        corpus. From this list, each group will select the <note place="comment"
                            resp="CO" n="6"><date when="2018-01-24T09:58:20Z"/><hi>Again, focusing
                                on representativeness this might lead to another canon corpus. The
                                MoU puts a focus on: “bases its research not on a small number of
                                representative and/or outstanding texts but on wide spectrum of the
                                literary production,”</hi></note> appropriate novels according to
                        the criteria of Doc1.</p>
                    <p> For this step, both the creation of the candidate list and the selection of
                        the final novels, we expect the advice of scholars and expert of each
                        literary tradition.</p>
                </div>
                <div>
                    <head>Step 2. Finding the novels.</head>
                    <p> The objective of this step is to obtain the novels selected in the previous
                        step in machine-readable format (as plain text). Applying selection criteria
                        from Doc1 it is possible that not all novels will be in a machine-readable
                        format or even digitized. In order to transform all texts to this format, we
                        suggest to follow the next steps.</p>
                    <p> First, to look for novels in digital repositories. See Appendix 2 for some
                        of them. In this case, however, it is important that the text retrieved
                        follows the sampling criteria of DOC1. Therefore, the novel must be in
                        machine-readable format (plain text, HTML, XML, DOC, ODT, RTF, Epub, or
                        similar.), it must be <hi rend="bold">licensed</hi> under open or free
                        licenses as Creative Commons or similar (according to MoU, the final corpus
                        will be freely available under Creative Commons License) and it must follow
                        the first edition of the novel. See Doc1 for all the sampling criteria.</p>
                    <p rend="Normal"><ref target="http://www.gutenberg.org/"> </ref>If it is not
                        possible to find a novel in a machine-readable format, the second option is
                        to look for it in digital libraries, traying to find a digitized version
                        (PDF format, JPG, or similar) that follows the sampling criteria of Doc1.
                        Finally, if the novel has not been digitized ever, it must be done in this
                        step (if we have resources to do it). We hope that this will be done only
                        for very few novels.<note place="comment" resp="LB" n="7"><date
                                when="2018-01-24T11:02:55Z"/><hi>I wonder though if we have the
                                resources to do this...</hi></note> In both cases, the text must
                        transform to a machine-readable format. See Appendix 3 for a list of tools
                        for digitizing texts. This way we will obtain a machine-readable format
                        (plain text) of each novel. The problem is that it is a complex and
                        time-consuming tasks.</p>
                    <p rend="Normal"><hi rend="bold"> Repository of raw texts.</hi> In order to make
                        a back-up of the corpus and the creation process, all novels will be stored
                        in a “raw text repository”, a text repository with the texts just like they
                        have been found or digitized.<note place="comment" resp="#Unknown_Author"
                            n="8"><date when="2018-01-24T10:58:22Z"/><hi>I agree that we should
                                archive the original format in which we received the texts. We
                                should also archive the toolchain we use to convert it to TEI
                                XML</hi></note></p>
                </div>
                <div>
                    <head>Step 3. Cleaning-up and normalizing texts.<note place="comment" resp="LB"
                            n="9"><date when="2018-01-24T10:59:15Z"/><hi>See
                                http://www.matthewjockers.net/2010/08/26/auto-converting-project-gutenberg-text-to-tei/</hi></note></head>
                    <p> Both if novels have been obtained directly in a machine-readable format or
                        if they have been digitized, it is necessary to check the text in order to
                        fix typos and normalize it. Exactly what elements must be normalized will be
                        decide later. </p>
                    <p> As far as possible, this task will be done automatically. However, a manual
                        review will be necessary.</p>
                    <p>
                        <anchor type="commentRangeStart" n="10"/><hi rend="bold">Open
                            question.</hi><anchor type="commentRangeEnd" n="10"/><note
                            place="comment" resp="CO" n="10"><date when="2018-01-24T10:16:10Z"
                                /><hi>Indeed, we may rely on automatic methods but I agree that some
                                manual checking might be necessary. We may discuss this with all
                                members as well? </hi></note>
                        <hi rend="bold">Both fixing and normalizing texts are complex and
                            time-consuming tasks. We have not funding to do them. However, the
                            reliability of the “distant” analysis (WG2) will depend on the quality
                            of these texts. We must decide how they will be done.</hi><note
                            place="comment" resp="BN" n="11"><date when="2018-01-25T10:35:21Z"
                                /><hi>I agree with Lou. We must offer novels always with their
                                metadata, so novels must include TEI annotation. In the same way, we
                                must offer a script to easily extract the plain text from the XML
                                file.</hi></note></p>
                </div>
                <div>
                    <head>Step 4. Annotation and encoding.</head>
                    <p> According to Doc2, three kinds of annotation must be developed: metadata
                        annotation, document structure annotation and linguistic/literary
                        annotation. The last one will not be done for the moment.</p>
                    <div>
                        <head>4.1. Metadata annotation.</head>
                        <p> Metadata attributes have been specified in the Encoding Scheme (Doc2).
                            In order to introduce in each text, two task must be done.</p>
                        <p> First, a controlled language must be developed to control the values of
                            each attribute and the coherence of the metadata.</p>
                        <p> Second, the annotation process of the metadata of each novel: to create
                            a well-formed XML document based on TEI and to introduce head attributes
                            with the appropriate value (metadata).</p>
                        <p> We will try to follow a semi-automatic annotation process here. Metadata
                            could be compiled and stored in a CSV file during previous steps (one
                            file per novel). Then a well-formed XML file could be created
                            automatically (Python script?) taking as an input the plain text of the
                            novel and the corresponding CSV file. At the end, all metadata will be
                            checked (see evaluation step).</p>
                    </div>
                    <div>
                        <head>4.2. Annotation of the structure of the novel.</head>
                        <p> Doc2 includes tags for representing the structure of the novel
                            (chapters, paragraphs, etc.).<note place="comment" resp="CO" n="12"
                                    ><date when="2018-01-24T10:20:39Z"/><hi>Lou, Borja, Do you know
                                    tools, annotating TEI automatically? Which tools can do which
                                    kind of annotation (e.g. one that is specialized for something,
                                    or some which rely on the picture of the pages?) This might be a
                                    topic to discuss with WG2, too. </hi></note><note
                                place="comment" resp="LB" n="13"><date when="2018-01-24T11:03:46Z"
                                    /><hi>Reply to Unbekannter Autor (24/01/2018, 10:20):
                                    "..."</hi><hi>Yes. See my link to Matt Jockers above, but there
                                    are plenty of others. It’s not difficult starting from
                                    reasonable HTML to produce a minimal TEI structure. Lots of
                                    people do it with lots of common tools (perl, XSLT, python,
                                    java…) </hi></note> Similarly, we will follow a semi-automatic
                            annotation approach, trying to annotate automatically as much as
                            possible.</p>
                        <p> However, in both cases, annotation must be manually checked with two
                            objectives: first, to introduce all that information that has not been
                            possible to annotate automatically (direct speech?, page breaks?,
                            tables?), and second to correct possible mistakes during automatic
                            annotation. At this moment, exactly what information will be annotated
                            has not been defined yet. See Doc2. </p>
                        <p>
                            <anchor type="commentRangeStart" n="14"/><hi rend="bold">Open
                                question.</hi><anchor type="commentRangeEnd" n="14"/><note
                                place="comment" resp="CO" n="14"><date when="2018-01-24T10:16:10Z"
                                    /><hi>Indeed, we may rely on automatic methods but I agree that
                                    some manual checking might be necessary. We may discuss this
                                    with all members as well? </hi></note>
                            <hi rend="bold">Manual revision and annotation are also complex and
                                time-consuming tasks. We must decide how they will be done.</hi></p>
                    </div>
                </div>
                <div>
                    <head>Step 5. Publication.</head>
                    <p> XML files must be publicly available in a specific repository. WG4 is
                        working to define the appropriate repository for the corpus. </p>
                    <p> Once a valid XML will be obtained, it will be published at the official
                        repository. Therefore, the corpus will be available during review/annotation
                        process.</p>
                    <p> The repository should guarantee that the novels could be downloaded as XML
                        files and as plain texts. We can use some technology as TEI-Publisher
                        (teipublisher.com/) or … <hi rend="background(yellow)">[LOU, please,
                            introduce here similar technologies that you commented during Skype
                            meeting.]</hi></p>
                    <p>
                        <anchor type="commentRangeStart" n="15"/>We can offer a DOI<anchor
                            type="commentRangeEnd" n="15"/><note place="comment" resp="CO" n="15"
                                ><date when="2018-01-24T10:39:09Z"/><hi>Maybe both</hi></note> for
                        each novel and for all the corpus through Zenodo: <ref
                            target="https://zenodo.org/"><hi rend="Internet_Link"
                                >https://zenodo.org/</hi></ref>.</p>
                </div>
                <div>
                    <head>Step 6. Evaluation.</head>
                    <p> Two aspects must be evaluated: text quality and annotation.</p>
                    <p> Text quality could be evaluated by checking if text samples follows the
                        criteria of Doc1. The idea here is to find recurrent mistakes. The
                        annotation the structure of each novel could be evaluated in the same way,
                        but in relation to Doc2. Metadata of each novel must be manually checked in
                        order to ensure the correctness of data.</p>
                    <p> Each file must be a valid XML-TEI document according to the defined XML
                        scheme. XML can be validated with standard tools as oXygen, Jing, XMLling,
                        the TEI-Validator (<ref
                            target="http://teibyexample.org/xquery/TBEvalidator.xq"><hi
                                rend="Internet_Link"
                                >http://teibyexample.org/xquery/TBEvalidator.xq</hi></ref>) or any
                        other tool.<note place="comment" resp="LB" n="16"><date
                                when="2018-01-24T11:07:29Z"/><hi>Nothing against this, but why
                                choose this validator in particular? We are defining an XML schema:
                                we can validate using normal XML tools (oXygen, jing,
                                xmllint...)</hi></note></p>
                    <p> Ideally, manual annotation must be evaluated following a double annotation
                        by two annotator and then calculating inter-annotators agreement. This
                        standard approach will show us the quality of the annotation scheme and the
                        annotation process. It is not clear at this moment which information will be
                        manually annotated. Therefore, we suggest to leave this point for later
                        (linguistic annotation).</p>
                </div>
                <div>
                    <head>Bibliography</head>
                    <p>Leech, Geoffrey (2005) “Adding Linguistic Annotation” in Wynne, M (editor),
                            <hi rend="italic">Developing Linguistic Corpora: a Guide to Good
                            Practice</hi>. Oxford: Oxbow Books. Available online from <ref
                            target="http://ota.ox.ac.uk/documents/creating/dlc/"><hi
                                rend="Internet_Link"
                                >http://ota.ox.ac.uk/documents/creating/dlc/</hi></ref></p>
                    <p>Stubbs, Amber and James Pustejovsky (2012) <hi rend="italic">Natural Language
                            Annotation for Machine Learning</hi>, O'Reilly.</p>
                </div>
                <div>
                    <head>APPENDIX 1. List of catalogs.</head>
                    <list type="unordered">
                        <item>OCLC WorldCat <ref target="https://www.oclc.org/"><hi
                                    rend="Internet_Link">https://www.oclc.org</hi></ref></item>
                        <item>…</item>
                    </list>
                </div>
                <div>
                    <head>APPENDIX 2. List of digital repositories</head>
                    <list type="unordered">
                        <item>Gutenberg project [several languages]:
                            http://www.gutenberg.org/</item>
                        <item>Oxford Text Archive (OTA) [several languages, mainly English]:
                            http://ota.ox.ac.uk/</item>
                        <item>Deutsches Textarchiv (DTA - German Text Archive) [German]:
                            http://www.bbaw.de/en/research/dta</item>
                        <item>TextGrid Repository [German]:
                            https://textgrid.de/en/digitale-bibliothek</item>
                        <item>OBVIL Bibliothèque [Frech]:
                            http://obvil.paris-sorbonne.fr/bibliotheque</item>
                        <item>ARTFL-FRANTEXT [French]:
                            http://artfl-project.uchicago.edu/content/artfl-frantext</item>
                        <item>Biblioteca Virtual Miguel de Cervantes [Spanish (Castilian, Catalan
                            and Galician)]: http://www.cervantesvirtual.com/</item>
                        <item>Liber Libri https://www.liberliber.it/online/ for Italian, or</item>
                        <item>Biblioteca Digital Camões
                            http://www.cvc.instituto-camoes.pt/conhecer/biblioteca-digital-camoes/literatura-1.html
                            for Portuguese.</item>
                        <item>Others...</item>
                    </list>
                </div>
                <div>
                    <head>Appendix 3. Tools and resources for digitizing texts.</head>
                    <list type="unordered">
                        <item>https://www.digitisation.eu/</item>
                        <item>http://www.impact-project.eu/index.php</item>
                        <item>https://www.digitisation.eu/tools-resources/tools-for-text-digitisation/</item>
                        <item>http://www.impact-project.eu/taa/tech/tools/</item>
                    </list>
                </div>
            </div>
        </body>
    </text>
</TEI>
